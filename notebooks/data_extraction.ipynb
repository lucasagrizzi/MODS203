{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp\n",
    "\n",
    "*Developed by Daniel Deutsch, José Lucas Barretto, Kevin Kühl and Lucas Miguel Agrizzi.*\n",
    "\n",
    "The present document is divided in 5 sections:\n",
    "\n",
    "- Part 1 - Extracting businesses's information\n",
    "- Part 2 - Extracting the number of fake reviews for each business\n",
    "- Part 3 - Extraction of fake reviews from restaurant pages\n",
    "- Part 4 - Extraction of real reviews\n",
    "- Final remarks\n",
    "\n",
    "We did an effort to present comments and explanations of each and every step. Please let us know if anything can have a better presentation or if there exists a better approach for a given step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Extracting businesses's information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import timedelta\n",
    "from requests_html import HTMLSession\n",
    "from requests_html import AsyncHTMLSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ARRONDISSEMENTS = json.load(open(\"./constants/arrondissements.json\", encoding=\"utf-8\"))\n",
    "CATEGORIES = json.load(open(\"./constants/categories.json\", encoding=\"utf-8\"))\n",
    "URL_DEFAULT_IMG = \"https://s3-media0.fl.yelpcdn.com/assets/srv0/yelp_styleguide/514f6997a318/assets/img/default_avatars/user_60_square.png\"\n",
    "\n",
    "# API authentication\n",
    "AUTHS_API = json.load(open(\"./constants/auth-api.json\", encoding=\"utf-8\")) \n",
    "available_auths_api = json.load(open(\"./constants/auth-api.json\", encoding=\"utf-8\"))\n",
    "\n",
    "# NordVPN possible countries\n",
    "countries = json.load(open(\"./constants/countries.json\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Category Parser\n",
    "\n",
    "The categories JSON provides from Yelp's site has some information we don't actually need. To make it easier for us to work with it, we will transform this variable into an array of categories ailias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_alias = [category[\"alias\"] for category in CATEGORIES if \"restaurants\" in category[\"parents\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Business Extraction\n",
    "\n",
    "Uses the Yelp API to gather all the possible restaurants in Paris. Since Paris is known to have more than 45,000 restaurants and the API only returns the first 1000 results given the parameters (with 50 results per page), we had to play with the request params to be able to gather as many restaurants as possible.\n",
    "\n",
    "The first option we tried was to iterate only on the arrondissements of Paris. This allowed us to retrieve 12204 restaurants.\n",
    "\n",
    "We wanted to check if we could get more restaurants, so we applied a second approach, which consisted of doing two iterations: one consisting of the categories each restaurant can have and the other on the arrondissements. This returned 13184 restaurants at the end. This last approach is presented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the HTTP request\n",
    "url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "headers = { \"Authorization\": f\"Bearer {random.choice(available_auths_api)['api_key']}\" }\n",
    "params = {\n",
    "    \"term\": \"restaurants\",\n",
    "    \"sort_by\": \"distance\",\n",
    "    \"open_now\": False, \n",
    "    \"offset\": 0,\n",
    "    \"limit\": 50, \n",
    "    \"location\": \"\",\n",
    "    \"categories\": \"\"\n",
    "}\n",
    "\n",
    "# Defines the time that the process started\n",
    "start = time.time()\n",
    "\n",
    "# Defines the dataframe of obtained restaurants\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Appends the restaurants to the dataframe\n",
    "for idx_arr, arr in enumerate(ARRONDISSEMENTS):\n",
    "\n",
    "    params[\"location\"] = f\"{arr}, Paris\"\n",
    "\n",
    "    for idx_cat, category in enumerate(categories_alias):        \n",
    "        \n",
    "        params[\"categories\"] = category\n",
    "        params[\"offset\"] = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            print(f\"\\rProgress: arrondissement {idx_arr+1}/{len(ARRONDISSEMENTS)}, category {idx_cat+1}/{len(categories_alias)}, available api accounts {len(available_auths_api)}/{len(AUTHS_API)}, restaurants {df.shape[0]}, time taken {timedelta(seconds=time.time()-start)}\", end=\"\")\n",
    "\n",
    "            r = requests.get(url, headers=headers, params=params)\n",
    "            status_code = r.status_code\n",
    "            r = r.json()\n",
    "\n",
    "            # Overflow on the amount of requests allowed per day\n",
    "            if status_code == 429:\n",
    "                available_auths_api = [keys for keys in available_auths_api if keys[\"api_key\"] != headers[\"Authorization\"].split(\" \")[1]]\n",
    "                if not available_auths_api:\n",
    "                    raise Exception(\"Out of valid api accounts for today\")\n",
    "                headers = { \"Authorization\": f\"Bearer {random.choice(available_auths_api)['api_key']}\" }\n",
    "\n",
    "            # Overflow on the limit (1000)\n",
    "            elif r.get(\"error\"):\n",
    "                break\n",
    "            \n",
    "            # Overflow on the amount of results\n",
    "            elif params[\"offset\"] > r[\"total\"]:\n",
    "                break\n",
    "            \n",
    "            # Got the results\n",
    "            else:\n",
    "                df = pd.concat([pd.DataFrame(r[\"businesses\"]), df], ignore_index=True)\n",
    "                params[\"offset\"] += params[\"limit\"]\n",
    "        \n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=[\"id\"], ignore_index=True)\n",
    "\n",
    "    # Saves to csv\n",
    "    df.to_csv(f\"datasets/arrondissements/raw_{arr}_restaurants.csv\")\n",
    "    \n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=[\"id\"], ignore_index=True)\n",
    "\n",
    "# Saves to csv\n",
    "df.to_csv(\"datasets/raw_restaurants.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Extracting the number of fake reviews for each business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Number of Fake Reviews Extraction\n",
    "\n",
    "Once we have the dataframe with all the restaurants, we need to obtain information over the reviews of each one. The problem is that Yelp's site blocks us after a certein amount of requests. The workaround is to use a VPN service that can give us anonymity, enabling us to scrape the information. So whenever we got blocked by Yelp, we iterated randomly over a list of countries and connected to a different option. This method will be used whenever the scraping method involves the possibility of being blocked.\n",
    "\n",
    "*This part of the code may not run on your computer due to the fact that we used the service NordVPN*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counter = pd.read_csv(\"datasets/raw_restaurants.csv\", index_col=0)\n",
    "\n",
    "country = \"France\"\n",
    "def fake_review_count(row, row_count):\n",
    "\n",
    "    global country   # It's global so it doesn't depend on the context \n",
    "\n",
    "    # Builds the url of the restaurant\n",
    "    url = f\"https://www.yelp.fr/not_recommended_reviews/{row['alias']}\"\n",
    "\n",
    "    # Tries to get the number of fake reviews\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        doc = html.fromstring(r.text)\n",
    "        count = int(doc.xpath('//*[@id=\"super-container\"]/div[2]/div/div/div[3]/div/div[1]/h3')[0].text.strip().split(\" \")[0])\n",
    "    \n",
    "    # Changes the country of the VPN and tries again\n",
    "    except:\n",
    "        country = random.choice(countries)\n",
    "        ! nordvpn connect {country} # Runs on the terminal\n",
    "        time.sleep(5)\n",
    "        fake_review_count(row, row_count)\n",
    "    \n",
    "    # Once we know the number of fake reviews we return it\n",
    "    else:\n",
    "        print(f\"\\rProgress: row {row.name}/{row_count}, accessing from: {country}\", end=\"\")\n",
    "        return count\n",
    "\n",
    "\n",
    "# Creates a column that shows the number of fake comments on the restaurant's page\n",
    "df_counter[\"fake_review_count\"] = df_counter.apply(lambda row: fake_review_count(row, df_counter.shape[0]), axis=1)\n",
    "\n",
    "df_counter.to_csv(\"datasets/raw_restaurants_fake_counts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2) Raw Data Storage\n",
    "\n",
    "Now that we gathered all the information we needed, we store them in a .csv file without any further treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/raw_restaurants_fake_counts.csv\", index_col=0)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=[\"id\"], ignore_index=True)\n",
    "\n",
    "# Saves to csv\n",
    "df.to_csv(\"./datasets/raw_complete_restaurants.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Data Processing\n",
    "\n",
    "Once we have the raw data, we fit it into a structure that is easier to work with when we are analyzing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1) Drop Unnecessary Columns\n",
    "\n",
    "Some of the business details returned by Yelp's API are not interesting for our goal. Therefore, there is no problem in dropping these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"phone\",\"display_phone\", \"is_closed\", \"image_url\", \"transactions\", \"name\", \"location\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2) Flattening Important Attributes\n",
    "\n",
    "The coordinates column is a dict with keys \"latitude\" and \"longitude\". It's harder to work with columns that are dicts in pandas. To avoid that, we create new columns based on these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_coordinates(x):\n",
    "    try:\n",
    "        json_obj = json.loads(x[\"coordinates\"].replace(\"'\", '\"'))\n",
    "        return json_obj['latitude'], json_obj['longitude']\n",
    "    except json.JSONDecodeError:\n",
    "        return pd.NA, pd.NA\n",
    "\n",
    "df[\"latitude\"] = df.apply(lambda x: flatten_coordinates(x)[0], axis=1)\n",
    "df[\"longitude\"] = df.apply(lambda x: flatten_coordinates(x)[1], axis=1)\n",
    "\n",
    "df.drop([\"coordinates\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3) Numericalizing Valuable Strings\n",
    "\n",
    "The price is proportional to the amount of $\\$$ returned by the API. It is harder to work with strings in this context, so we set the price column as the number of $\\$$ returned by the API instead of a string with $\\$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(row):\n",
    "    return len(str(row[\"price\"]))\n",
    "\n",
    "df[\"price\"] = df.apply(lambda row: numericalize(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.4) Dropping Unnecessary Categories Attributes\n",
    "\n",
    "The categories column is an array of dictionaries. These dictionaries have \"alias\" and \"title\" as their keys. We will only use the \"alias\", so we can make the column be an array of alias instead an array of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categ(row):\n",
    "    categories = []\n",
    "    text = row[\"categories\"].replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    text = text.split(\",\")\n",
    "    for item in text:\n",
    "        final = item.replace(\"'\", '\"').strip().replace('\"', \"\").replace(\"{\",\"\").replace(\"}\", \"\").split(\":\")\n",
    "        if final[0] == \"alias\":\n",
    "            categories.append(final[1])\n",
    "    return categories\n",
    "        \n",
    "df[\"categories\"] = df.apply(lambda row: get_categ(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Final Error Check\n",
    "In this part, we search for missing information on the number of fake reviews and try once again to capture them. This make sure we eliminate inconsistent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    if pd.isnull(row[\"fake_review_count\"]):\n",
    "        url = f\"https://www.yelp.fr/not_recommended_reviews/{row['alias']}\"\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            doc = html.fromstring(r.text)\n",
    "            count = int(doc.xpath('//*[@id=\"super-container\"]/div[2]/div/div/div[3]/div/div[1]/h3')[0].text.strip().split(\" \")[0])\n",
    "            df.at[i,\"fake_review_count\"] = count\n",
    "        except:    \n",
    "            country = random.choice(countries)\n",
    "            ! nordvpn connect {country} # Runs on the terminal\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5) Processed Data Storage\n",
    "\n",
    "Once we have the data fitting the structure that we wanted, we store it in a different .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"datasets/processed_restaurants.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6) Notes on Obtained Dataset\n",
    "\n",
    "The data saved in \"processed_restaurants.csv\" contains the following columns:\n",
    "\n",
    "- **id**: restaurant's id, unique identifier code for a given restaurant\n",
    "- **alias**: unique name identifier (used to compose with yelp.fr url to form restaurant own webpage)\n",
    "- **url**: restaurant's webpage url\n",
    "- **review_count**: number of reviews on a given restaurant's page\n",
    "- **categories**: list of categories a restaurant belongs to\n",
    "- **rating**: general rating of a given restaurant\n",
    "- **distance**: distance relative to a point in the center of the arrondissement the restaurant belongs to\n",
    "- **price**: price index for the given restaurant\n",
    "- **fake_review_count**: number of fake reviews on a given restaurant's page\n",
    "- **latitude**: latitude of the restaurant's position\n",
    "- **longitude**: longitude of the restaurant's position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Extraction of fake reviews from restaurant pages\n",
    "\n",
    "In order to extract the fake reviews we applied an approach in which we selected the first 10 fake reviews of each restaurant in our dataset. If the restaurant had less than 10 fake reviews, we collect all the reviews.\n",
    "\n",
    "At the end, we were able to retrieve 32869 fake reviews from a total of 40965. This represents 80.23% of the total number of fake reviews available.\n",
    "\n",
    "## 3.1) Extraction of the fake reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the dataset containing information from the obtained restaurants\n",
    "df_restaurants = pd.read_csv(\"datasets/processed_restaurants.csv\", index_col=0)\n",
    "\n",
    "# Create an empty dataframe to store reviews\n",
    "df_reviews = pd.DataFrame(columns=['user_origin', 'user_friends_count', 'user_reviews_count', 'is_fake', 'date', 'rest_alias', 'text', 'rating', 'has_img', 'reviews_have_photos'])\n",
    "\n",
    "# Creating useful variables\n",
    "row_count = df.shape[0]\n",
    "progress = 0\n",
    "total_reviews = 0\n",
    "\n",
    "# Iterating over each row of the dataframe of restaurants\n",
    "for index, row in df_restaurants.iterrows():\n",
    "    iterator = 0\n",
    "    less10 = 0\n",
    "    # Getting the number of fake reviews for the current restaurant\n",
    "    number_fake_reviews = int(row[\"fake_review_count\"])\n",
    "    \n",
    "    # Check if there are less than 10 reviews (all reviews would be in a single page)\n",
    "    if number_fake_reviews < 10:\n",
    "        less10 = 1\n",
    "        \n",
    "    # Update progress counter if the restaurant has no fake reviews (and do not enter the while loop below)\n",
    "    if number_fake_reviews == 0:\n",
    "        progress += 1\n",
    "        \n",
    "    # Builds the url of the restaurant\n",
    "    url = f\"https://www.yelp.fr/not_recommended_reviews/{row['alias']}\"\n",
    "\n",
    "    # Tries to get the fake reviews\n",
    "    # Inside a while loop to treat all possible exceptions\n",
    "    while(number_fake_reviews != 0):\n",
    "        try:\n",
    "            # Do the request\n",
    "            r = requests.get(url)\n",
    "            # Parse the reponse from the request\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            # Get all the <li> elements from the page (those contains the reviews)\n",
    "            reviews_in_page = soup.find_all(\"ul\")[0].find_all(\"li\")\n",
    "            \n",
    "            # Iterate over the first 10 fake reviews of the current analysed restaurant\n",
    "            for i in range(10 if not less10 else number_fake_reviews):\n",
    "                # First, we analyse if there is a icon for posted photos by the user\n",
    "                # This changes the way we iterate between one review and another\n",
    "                # As it adds an extra <li> block to be counted\n",
    "                # It indicates how many photos the user has already posted in Yelp\n",
    "                reviews_have_photos = True if len(reviews_in_page[iterator].find_all(class_=\"photo-count responsive-small-display-inline-block\"))>0 else False\n",
    "                # Initializing an empty dictionary to contain the review\n",
    "                review = {}\n",
    "                # First filling it with the user's origin\n",
    "                review[\"user_origin\"] = reviews_in_page[iterator].find_all(\"b\")[0].text\n",
    "                # Sometimes Yelp uses Membre Qype and Membre Cityvox in the alias of a given user\n",
    "                # When this is the case, we have to ignore the first response for the user origin and\n",
    "                # Start capturing from the second entry (<b> blocks)\n",
    "                if review[\"user_origin\"] == \"Membre Qype\" or review[\"user_origin\"]==\"Membre Cityvox\":\n",
    "                    review[\"user_origin\"] = reviews_in_page[iterator].find_all(\"b\")[1].text\n",
    "                    review[\"user_friends_count\"] = reviews_in_page[iterator].find_all(\"b\")[2].text\n",
    "                    review[\"user_reviews_count\"] = reviews_in_page[iterator].find_all(\"b\")[3].text\n",
    "                else:\n",
    "                    review[\"user_friends_count\"] = reviews_in_page[iterator].find_all(\"b\")[1].text\n",
    "                    review[\"user_reviews_count\"] = reviews_in_page[iterator].find_all(\"b\")[2].text\n",
    "                # Indicates this is a fake review (to use when comparing with real reviews)\n",
    "                review[\"is_fake\"] = True\n",
    "                review[\"date\"] = reviews_in_page[iterator].find_all(\"span\", class_=\"rating-qualifier\")[0].text.strip()\n",
    "                review[\"rest_alias\"] = f\"{row['alias']}\"\n",
    "                review[\"text\"] = reviews_in_page[iterator].find_all(\"p\")[0].text\n",
    "                review[\"rating\"] = reviews_in_page[iterator].find_all(\"img\", class_=\"offscreen\")[0].attrs[\"alt\"].split(\" \")[0]\n",
    "                review[\"has_img\"] = reviews_in_page[iterator].find_all(\"img\", class_=\"offscreen\")[0].attrs[\"src\"] != URL_DEFAULT_IMG\n",
    "                review[\"reviews_have_photos\"] = reviews_have_photos\n",
    "                # Add it to the dataframe\n",
    "                df_reviews = df_reviews.append(review.copy(), ignore_index=True)\n",
    "                # Update the iterator\n",
    "                if reviews_have_photos:\n",
    "                    iterator += 6\n",
    "                else:\n",
    "                    iterator += 5\n",
    "        # This never happens, but we wanted to make a stable program\n",
    "        except KeyError:\n",
    "            progress+=1\n",
    "            break\n",
    "        # All possible exceptions will be related to conections to Yelp's website\n",
    "        # We solve them by changing to another country\n",
    "        except Exception:\n",
    "            country = random.choice(countries)\n",
    "            # Runs on the terminal\n",
    "            ! nordvpn connect {country}\n",
    "            time.sleep(5)\n",
    "        # If there is no exceptions, we update the progress counter\n",
    "        # Also updating the total number of reviews retrieved and printing the status\n",
    "        else:\n",
    "            progress += 1\n",
    "            total_reviews += 10 if not less10 else number_fake_reviews\n",
    "            print(f\"\\rProgress: row {progress}/{row_count}, accessing from: {country}, Total reviews: {total_reviews}\", end=\"\")\n",
    "            break\n",
    "\n",
    "# Saving the dataframe to a csv file\n",
    "df_reviews.to_csv(\"datasets/raw_fake_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Notes on Obtained Dataset\n",
    "\n",
    "The data saved in \"raw_fake_reviews.csv\" contains the following columns:\n",
    "\n",
    "- **user_origin**: Origin of the user that posted the review\n",
    "- **user_friends_count**: Number of friends the user has\n",
    "- **user_reviews_count**: Number of reviews the user has already posted\n",
    "- **is_fake**: Tag that indicates if the given review is fake or not\n",
    "- **date**: Date on which the review was posted\n",
    "- **rest_alias**: Alias of the restaurant object from the review\n",
    "- **text**: The text of the review\n",
    "- **rating**: The rating attributed on the analysed review\n",
    "- **has_img**: Tag that indicates if the user has a profile image\n",
    "- **reviews_have_photos**: Tag that indicates if the user usually posts photos on Yelp (for other reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Extraction of real reviews\n",
    "\n",
    "For this last part of extraction we were able to use a hidden api from Yelp's website.\n",
    "\n",
    "We iterate over different parameters and we are able to retrieve all the real reviews from Yelp for the restaurants obtained in Part 1.\n",
    "\n",
    "At the end, we were able to retrieve a total of 226,143 real reviews after 18h48min26s of execution.\n",
    "\n",
    "## 4.1) Reviews Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the dataset containing information from the obtained restaurants\n",
    "df_restaurants = pd.read_csv(\"./datasets/processed_restaurants.csv\", index_col=0)\n",
    "\n",
    "# Defines the time that the process started\n",
    "start = time.time()\n",
    "\n",
    "# Defines the reviews dataframe\n",
    "df_reviews = pd.DataFrame()\n",
    "\n",
    "# Iterating through the rows of the dataframe\n",
    "for idx, row in df_restaurants.iterrows():\n",
    "    for rl in [\"en\", \"fr\"]:\n",
    "    \n",
    "        # Sets the HTTP request\n",
    "        url = f\"https://www.yelp.com/biz/{row['id']}/review_feed\"\n",
    "        params = {\n",
    "            \"rl\": rl,\n",
    "            \"sort_by\": \"relevance_desc\",\n",
    "            \"start\": 0\n",
    "        }\n",
    "\n",
    "        while True:\n",
    "\n",
    "            print(f\"\\rProgress: restaurants {row.name+1}/{df_restaurants.shape[0]}, reviews {df_reviews.shape[0]}, time taken {timedelta(seconds=time.time()-start)}\", end=\"\")\n",
    "\n",
    "            # Makes the HTTP request\n",
    "            try:\n",
    "                r = requests.get(url, params=params)\n",
    "\n",
    "            # Good response from the API\n",
    "                if r.status_code == 200:\n",
    "\n",
    "                    # Obtains the reviews of this page\n",
    "                    reviews = r.json()[\"reviews\"]\n",
    "\n",
    "                    # Still have reviews from this restaurant\n",
    "                    if reviews:\n",
    "                        df_reviews = pd.concat([pd.DataFrame(reviews), df_reviews], ignore_index=True)\n",
    "                        df_reviews[\"is_fake\"] = False\n",
    "                        params[\"start\"] += 20\n",
    "                \n",
    "                    # Overflow on restaurant's reviews (go to the next restaurant)\n",
    "                    else:\n",
    "                        break\n",
    "                elif r.status_code == 503:\n",
    "                    raise Exception\n",
    "            # Our IP got blocked from the API\n",
    "            except Exception:\n",
    "                country = random.choice(countries)\n",
    "                ! nordvpn connect {country} # Runs on the terminal\n",
    "                time.sleep(5)\n",
    "\n",
    "\n",
    "# Saves the obtained reviews into multiple csv files\n",
    "chunks = np.array_split(df_reviews, 7)\n",
    "\n",
    "for i in range(len(chunks)):\n",
    "    chunks[i].to_csv(\"datasets/raw_reviews_{}.csv\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Notes on Obtained Dataset\n",
    "The data saved in \"raw_reviews.csv\" contains the following columns:\n",
    "\n",
    "- **comment**: The text of the review (can be extracted from the json format)\n",
    "- **rating**: The rating attributed on the analysed review\n",
    "- **photosUrl**: Internal URL to Yelp (remembering we extracted it from a hidden API)\n",
    "- **feedback**: User's return on Yelp's standard reactions to a given restaurant (useful, funny, cool...)\n",
    "- **business**: JSON containing the informations about the restaurant object from the review\n",
    "- **localizedDateVisited**: Empty column. Would represent the date from the user's visit to the restaurant\n",
    "- **businessOwnerReplies**: Replies from the business owner to the given review\n",
    "- **userId**: Unique identifier code for the user\n",
    "- **previousReviews**: All the previous reviews from the given user\n",
    "- **lightboxMediaItems**: Important JSON containing information such as the number of reviews the user has already done and the number of friends (easy to access information for being in JSON format)\n",
    "- **photos**: Posted photos on the given review\n",
    "- **tags**: Those are some tags returned by the API, they are redundant to other information previous described and sometimes they describe internal properties of the review\n",
    "- **isUpdated**: Indicates that the present review is an update of a previous review of the same user on the same restaurant\n",
    "- **user**: JSON containing the user's information\n",
    "- **appreciatedBy**: Contains information of users that feel helped with the given review\n",
    "- **totalPhotos**: Total number of photos posted for the given review\n",
    "- **id**: Review's unique id\n",
    "- **localizedDate**: Date when the review was posted\n",
    "- **is_fake**: Tag to indicate the review is fake or not (in this case, it is false for real reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final remarks\n",
    "\n",
    "At the end of the process of extraction we were able to retrieve a good amount of information to be used in our analysis.\n",
    "\n",
    "Some of our data (specially the last part, which was captured from Yelp's hidden API) needs to be reformated to better serve the purpose of the analysis part. We note that this will be a very simple task as we have already pre selected in which format we wanted each column. Some contains the direct value while others contain a JSON, which allows for an easy value extraction.\n",
    "\n",
    "This will be surely be finished before the analysis and visualization part.\n",
    "\n",
    "We are overall proud of the presented work as we could test a very broad sets of tools learned in class. Also, we are excited for the future work on the analysis of the collected that and the presentation of useful visualization ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
